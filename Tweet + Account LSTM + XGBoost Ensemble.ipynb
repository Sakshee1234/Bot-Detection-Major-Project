{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1aJoqZfAr8VssZa3GBlaLKC99yTI41iWp","timestamp":1713680414958}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **TWEET + ACCOUNT FEATURE LSTM**"],"metadata":{"id":"nmt4Lirv-iFR"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"JuV4uHp2ooAH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1727606736268,"user_tz":-330,"elapsed":33416,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"f0653ac2-8f92-48bd-ae14-4eff1bafb2d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install emoji Levenshtein emot ttp wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBNrtP-X_y4A","executionInfo":{"status":"ok","timestamp":1727597861105,"user_tz":-330,"elapsed":21877,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"4f68a6f2-0a60-49c3-9ba5-6014bfd4c4eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-2.13.2-py3-none-any.whl.metadata (5.8 kB)\n","Collecting Levenshtein\n","  Downloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n","Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl.metadata (396 bytes)\n","Collecting ttp\n","  Downloading ttp-0.9.5-py2.py3-none-any.whl.metadata (9.6 kB)\n","Collecting wandb\n","  Downloading wandb-0.18.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n","  Downloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n","Downloading emoji-2.13.2-py3-none-any.whl (553 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading levenshtein-0.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ttp-0.9.5-py2.py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wandb-0.18.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: emot, ttp, smmap, setproctitle, sentry-sdk, rapidfuzz, emoji, docker-pycreds, Levenshtein, gitdb, gitpython, wandb\n","Successfully installed Levenshtein-0.26.0 docker-pycreds-0.4.0 emoji-2.13.2 emot-3.1 gitdb-4.0.11 gitpython-3.1.43 rapidfuzz-3.10.0 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 ttp-0.9.5 wandb-0.18.2\n"]}]},{"cell_type":"code","source":["import os\n","import random\n","import gdown\n","import gensim\n","import gensim.downloader as gloader\n","import pandas as pd\n","import torch\n","from gensim.models import KeyedVectors\n","from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n","from argparse import ArgumentParser\n","from datetime import datetime\n","import pytz\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.ensemble import RandomForestClassifier\n","import wandb\n","import time\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch import Tensor\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import json\n","from collections import OrderedDict, namedtuple\n","import torch.nn.utils.rnn as rnn\n","from torch.utils.data import DataLoader, Dataset\n","import collections\n","import math\n","import os\n","import re\n","from datetime import datetime\n","from statistics import mean\n","from string import punctuation\n","import emoji\n","import Levenshtein\n","import nltk\n","import pandas as pd\n","from emot.core import emot\n","from nltk.corpus import stopwords\n","from nltk.tokenize import TweetTokenizer\n","from pandas.core.common import flatten\n","from sklearn.preprocessing import StandardScaler\n","from ttp import ttp\n","from sklearn.metrics import confusion_matrix\n"],"metadata":{"id":"sFYG35px6d7X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 128                   # number of sentences in each mini-batch\n","LR = 1e-3                          # learning rate\n","NUM_EPOCHS = 10                    # number of epochs\n","WEIGHT_DECAY = 1e-4                # regularization\n","LSTM_HIDDEN_DIM = 300              # hidden dimension of lstm network\n","LSTM_NUM_LAYERS = 1                # num of recurrent layers of lstm network\n","FREEZE = False                     # wheter to make the embedding layer trainable or not\n","DROPOUT = True                     # wheter to use dropout layer or not\n","DROPOUT_P = 0.5                    # dropout probability\n","EMBEDDING_MODEL_NAME = 'fastText'  # which embedding model to use\n","NUM_TW_FEATURES = 30               # how many tweet from the same user are exploited to compute metadata features\n","NUM_TW_TXT = 10\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"GmtpZWbkAG1K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = {\n","'batch_size' : BATCH_SIZE,\n","'lr' : LR,\n","'num_epochs' : NUM_EPOCHS,\n","'weight_decay' : WEIGHT_DECAY,\n","'lstm_hidden_dim' : LSTM_HIDDEN_DIM,\n","'lstm_num_layers': LSTM_NUM_LAYERS,\n","'freeze' : FREEZE,\n","'dropout' : DROPOUT,\n","'dropout_p' : DROPOUT_P,\n","'device' : DEVICE,\n","'emb_model_name' : EMBEDDING_MODEL_NAME,\n","'num_tw_features' : NUM_TW_FEATURES,\n","'num_tw_txt' : NUM_TW_TXT\n","}"],"metadata":{"id":"GPw57VLS--na"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_df = pd.read_pickle('/content/drive/MyDrive/Major Project/Different combinations/data/processed_dataset_v4.pkl')"],"metadata":{"id":"-Oz0mD-n6eWC","executionInfo":{"status":"ok","timestamp":1727606766243,"user_tz":-330,"elapsed":6483,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["dataset = pd.read_pickle('/content/drive/MyDrive/Major Project/Different combinations/data/processed_dataset_v4.pkl')"],"metadata":{"id":"3ytZ5cdI7CJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_model_cached_path = \"/content/drive/MyDrive/Major Project/Different combinations/data/twitter-multilingual-300d.new.bin\"\n","# gdown.download(id=\"1DprdHGocFXJ9swnb2pDJJxHw5QR810LS\",output=str(emb_model_cached_path))"],"metadata":{"id":"qFgHOw9zBmZW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emb_model = KeyedVectors.load_word2vec_format(emb_model_cached_path, binary=True)\n","print('vectors loaded')"],"metadata":{"id":"23r3h4Z1CUjb","executionInfo":{"status":"ok","timestamp":1727598032310,"user_tz":-330,"elapsed":118362,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa6ac277-3369-48ef-e405-0ad9e12206a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["vectors loaded\n"]}]},{"cell_type":"code","source":["text_features = ['avg_length','avg_cleaned_length','1+_mention','1+_emot','1+_url','max_hashtag','max_mentions','url_count','hashtag_count','mention_count',\n","    'emot_count','punct_count','rt_count','unique_hashtag_ratio','unique_rt_ratio','unique_words_ratio']\n","\n","account_features = ['has_location', 'has_url', 'name_len', 'screen_name_len', 'description_len', 'followings_count','followers_count',\n","    'fofo_ratio', 'tweets_count', 'listed_count', 'num_in_screen_name', 'hashtag_in_description', 'def_profile', 'screen_name_entropy', 'tweet_freq', 'is_geo_enabled',\n","    'favourites_count', 'lev_dist_name_screeName', 'followers_growth_rate', 'followings_growth_rate', 'favourites_growth_rate']"],"metadata":{"id":"YiZBOd8PDEoT","executionInfo":{"status":"ok","timestamp":1727606802634,"user_tz":-330,"elapsed":494,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["dataset_df['text_features'] = dataset_df[text_features].values.tolist()\n","dataset_df['account_features'] = dataset_df[account_features].values.tolist()\n","text_features_dim = len(text_features)\n","account_features_dim = len(account_features)"],"metadata":{"id":"WTsRtjETDdDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TweetAndMultiFeaturesDataset(Dataset):\n","\n","    def __init__(self, dataframe: pd.DataFrame):\n","        self.tweet = dataframe['processed_tweet']\n","        self.label = dataframe['label']\n","        self.txt_features = dataframe['text_features']\n","        self.acc_features = dataframe['account_features']\n","\n","    def __len__(self):\n","        return len(self.label)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'tweet': self.tweet[idx],\n","            'label': self.label[idx],\n","            'txt_features': self.txt_features[idx],\n","            'acc_features': self.acc_features[idx]\n","            }"],"metadata":{"id":"wQB0SGGaENc9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = TweetAndMultiFeaturesDataset(dataset_df[dataset_df['split'] == 'train'].reset_index(drop=True))\n","val_ds = TweetAndMultiFeaturesDataset(dataset_df[dataset_df['split'] == 'val'].reset_index(drop=True))\n","test_ds = TweetAndMultiFeaturesDataset(dataset_df[dataset_df['split'] == 'test'].reset_index(drop=True))"],"metadata":{"id":"rKqgXGlYDzyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Vocab = namedtuple('Vocabulary',['word2int','int2word','unique_words'])"],"metadata":{"id":"wNR-FetsFSUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Building vocab...')\n","\n","unique_words : list = dataset_df['processed_tweet'].explode().unique().tolist()\n","unique_words.insert(0,'<pad>')\n","unique_words.insert(1,'<unk>')\n","\n","word2int = OrderedDict()\n","int2word = OrderedDict()\n","\n","for i, word in enumerate(unique_words):\n","    word2int[word] = i\n","    int2word[i] = word\n","\n","vocab = Vocab(word2int,int2word,unique_words)\n","\n","print(f'the number of unique words is {len(unique_words)}')"],"metadata":{"id":"I13L2jGnEZg6","executionInfo":{"status":"ok","timestamp":1727598062864,"user_tz":-330,"elapsed":862,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"45a4fffe-828c-43cc-ff3a-5721d6cd05a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building vocab...\n","the number of unique words is 111542\n"]}]},{"cell_type":"code","source":["print('Building embedding matrix...')\n","\n","embedding_dimension = emb_model.vector_size #how many numbers each emb vector is composed of\n","embedding_matrix = np.zeros((len(vocab.word2int)+1, embedding_dimension), dtype=np.float32)   #create a matrix initialized with all zeros\n","\n","for word, idx in vocab.word2int.items():\n","    if idx == 0: continue\n","    try:\n","        embedding_vector = emb_model[word]\n","    except (KeyError, TypeError):\n","        embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n","\n","    embedding_matrix[idx] = embedding_vector     #assign the retrived or the generated vector to the corresponding index\n","\n","emb_matrix = embedding_matrix\n","\n","print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"],"metadata":{"id":"j43uGzaUFZgq","executionInfo":{"status":"ok","timestamp":1727598068280,"user_tz":-330,"elapsed":829,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f331d996-1c84-45ca-d01c-9df598ed878d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Building embedding matrix...\n","Embedding matrix shape: (111543, 300)\n"]}]},{"cell_type":"code","source":["model_cfg = {\n","  'pad_idx' : vocab.word2int['<pad>'],\n","  'freeze_embedding' : FREEZE,\n","  'dropout' : DROPOUT,\n","  'dropout_p' : DROPOUT_P,\n","  'hidden_dim' : LSTM_HIDDEN_DIM,\n","  'num_layers': LSTM_NUM_LAYERS,\n","  'txt_features_dim' : text_features_dim,\n","  'acc_features_dim' :account_features_dim\n","}"],"metadata":{"id":"Rp5OlXaxFnpt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cfg = model_cfg\n","device = DEVICE\n","\n","matrix = torch.from_numpy(emb_matrix).to(DEVICE)   #the embedding matrix\n","_ , word_embedding_dim = matrix.shape\n","\n","embedding_layer=nn.Embedding.from_pretrained(matrix, freeze=cfg['freeze_embedding'], padding_idx = cfg['pad_idx'])\n","\n","lstm = nn.LSTM(word_embedding_dim, cfg['hidden_dim'], num_layers = cfg['num_layers'], batch_first = True, bidirectional = True)\n","\n","dropout = nn.Dropout(cfg['dropout_p'])\n","\n","linear1 = nn.Linear(cfg['txt_features_dim'] + cfg['hidden_dim']*2,cfg['hidden_dim'])\n","linear2 = nn.Linear(cfg['acc_features_dim'] + cfg['hidden_dim'],1)"],"metadata":{"id":"O9g4ThavGEVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = dataset_df[dataset_df['split']=='train']\n","counts = train['label'].value_counts().to_dict()\n","print(counts)\n","\n","(human, bot) = counts[0.0], counts[1.0]\n","weight_positive_class = torch.tensor([human/bot], device = DEVICE)  #weight to give to positive class"],"metadata":{"id":"0BnAuUEsHgnx","executionInfo":{"status":"ok","timestamp":1727598078122,"user_tz":-330,"elapsed":661,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1943fbaf-8f7a-4ae0-aa68-95b825d80ed6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{1.0: 4077, 0.0: 3312}\n"]}]},{"cell_type":"code","source":["criterion = nn.BCEWithLogitsLoss(pos_weight=weight_positive_class)"],"metadata":{"id":"UzHFZ-FyIQvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parameters = list(embedding_layer.parameters()) + list(lstm.parameters()) + list(dropout.parameters()) + \\\n","              list(linear1.parameters()) + list(linear2.parameters())"],"metadata":{"id":"Nk8d2PO2Tz2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(parameters, lr=LR, weight_decay=WEIGHT_DECAY)"],"metadata":{"id":"KPIfFHYrIRe0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attr = namedtuple('attr', ['device', 'dataset', 'train_ds', 'val_ds', 'test_ds'])"],"metadata":{"id":"dlDt35UPVysK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attr = attr(device=device, dataset=dataset_df.copy(deep=True), train_ds=train_ds, val_ds=val_ds, test_ds=test_ds)"],"metadata":{"id":"9aZoFRAQWY3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_collate(batch):\n","    device = DEVICE\n","\n","    tweet_lengths = torch.tensor([len(example['tweet']) for example in batch])\n","\n","    txt_features = torch.tensor([example['txt_features'] for example in batch], device=device)\n","    acc_features = torch.tensor([example['acc_features'] for example in batch], device=device)\n","\n","    numerized_tweets = [torch.tensor([vocab.word2int.get(token, vocab.word2int['<unk>']) for token in example['tweet']]) for example in batch]\n","    padded_tweets = rnn.pad_sequence(numerized_tweets, batch_first=True, padding_value=vocab.word2int['<pad>']).to(device)\n","\n","    labels = torch.tensor([example['label'] for example in batch], device=device)\n","\n","    return {\n","        'tweets': padded_tweets,\n","        'txt_features': txt_features,\n","        'acc_features': acc_features,\n","        'labels': labels,\n","        'lengths': tweet_lengths\n","    }\n"],"metadata":{"id":"g7aEbhlKXRu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def getDataloader(split : str, batch_size : int, shuffle : bool):\n","#     dataset = getattr(attr,split+'_ds')\n","#     return DataLoader(dataset,batch_size,shuffle=shuffle,collate_fn=custom_collate)"],"metadata":{"id":"oHuxxKYHIzS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate)\n","val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n","test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)"],"metadata":{"id":"yP7GenexIRbd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FS_i1wwRtreT","executionInfo":{"status":"ok","timestamp":1727598101509,"user_tz":-330,"elapsed":3,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"2d848b72-8bbd-4069-ea6c-1bcd6cf8ef62"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.utils.data.dataloader.DataLoader at 0x7a485f728a90>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["test_ds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mBL5kIzctv3U","executionInfo":{"status":"ok","timestamp":1727598108506,"user_tz":-330,"elapsed":536,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"2813cac1-21e1-42ad-fe64-d41abfcae3f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.TweetAndMultiFeaturesDataset at 0x7a4ae029ec50>"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["def metrics(y_true, y_pred):\n","\n","    acc = accuracy_score(y_true, y_pred)\n","\n","    f1 = f1_score(y_true,y_pred,average='macro')\n","\n","    prec = precision_score(y_true,y_pred,average='macro')\n","\n","    rec = recall_score(y_true,y_pred,average=\"macro\")\n","\n","    return acc, f1, prec, rec"],"metadata":{"id":"ZEfphKYZJjjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TweetAndAccount_model(nn.Module):\n","\n","    def __init__(self, emb_matrix: np.ndarray, cfg : dict, device) :\n","        super().__init__()\n","\n","        self.cfg = cfg\n","        self.device = device\n","\n","        self.embedding_layer, self.word_embedding_dim = self.build_emb_layer(emb_matrix,cfg['pad_idx'], cfg['freeze_embedding'])\n","\n","        self.lstm = nn.LSTM(self.word_embedding_dim, cfg['hidden_dim'], num_layers = cfg['num_layers'], batch_first = True, bidirectional = True)\n","\n","        self.dropout = nn.Dropout(cfg['dropout_p'])\n","\n","        self.linear1 = nn.Linear(cfg['txt_features_dim'] + cfg['hidden_dim']*2,cfg['hidden_dim'])\n","        self.linear2 = nn.Linear(cfg['acc_features_dim'] + cfg['hidden_dim'],1)\n","\n","\n","    def name(self):\n","        return 'TweetAndAccount_model'\n","\n","    def build_emb_layer(self, weights_matrix: np.ndarray, pad_idx : int, freeze : bool):\n","\n","        matrix = torch.from_numpy(weights_matrix).to(self.device)   #the embedding matrix\n","        _ , embedding_dim = matrix.shape\n","\n","        emb_layer = nn.Embedding.from_pretrained(matrix, freeze=freeze, padding_idx = pad_idx)   #load pretrained weights in the layer and make it non-trainable (TODO: trainable ? )\n","\n","        return emb_layer, embedding_dim\n","\n","\n","    def forward(self, batch_data):\n","\n","        tweets = batch_data['tweets']           # [batch_size, num_tokens]\n","        tweet_lengths = batch_data['lengths']   # [batch_size]\n","\n","        #embed each word in a sentence with a n-dim vector\n","        word_emb_tweets = self.embedding_layer(tweets)  # word_emb_tweets = [batch_size, num_tokens, embedding_dim]\n","\n","        #pass the embedded tokens throught lstm network\n","        packed_embeddings = pack_padded_sequence(word_emb_tweets, tweet_lengths, batch_first=True, enforce_sorted=False) #tweet_lengths.cpu() TODO\n","        output, (hn,cn)  = self.lstm(packed_embeddings)   # hn = [2, batch_size, hidden_dim]\n","\n","        #concat forward and backward output\n","        fwbw_hn = torch.cat((hn[-1,:,:],hn[-2,:,:]),dim=1)  # fwbw_hn = [batch_size, 2*hidden_dim]\n","\n","        if self.cfg['dropout']:\n","            fwbw_hn = self.dropout(fwbw_hn)\n","\n","\n","        out = torch.cat([fwbw_hn,batch_data['txt_features']],dim=1)     # out = [batch_size, hidden_dim*2 + txt_features_dim]\n","        out = self.linear1(out)                                         # out = [batch_size, hidden_dim]\n","        out = F.relu(out)\n","        out = torch.cat([out,batch_data['acc_features']],dim=1)         # out = [batch_size, hidden_dim + acc_features_dim]\n","        out = self.linear2(out)                                         # out = [batch_size, 1]\n","\n","        return out"],"metadata":{"id":"oF4-vgh14gtU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Trainer():\n","\n","    def __init__(self, model : nn.Module, device, criterion, optimizer) : #TODO qualcosa\n","\n","        self.device = device\n","\n","        model.to(self.device)\n","        self.model = model\n","\n","        self.criterion = criterion.to(self.device) if isinstance(criterion, nn.Module) else criterion\n","        self.optimizer = optimizer\n","\n","        # self.models_dir = glob.BASE_PATH / 'models'\n","\n","\n","    def train_loop(self, dataloader : DataLoader):\n","\n","        batch_size = dataloader.batch_size\n","        dataset_size = len(dataloader.dataset)\n","\n","        start_time = time.perf_counter()\n","\n","        tot_loss = 0\n","\n","        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors\n","        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n","\n","        self.model.train()\n","\n","        for batch_id, batch_data in enumerate(tqdm(dataloader)):\n","\n","            self.optimizer.zero_grad()\n","\n","            predictions : Tensor = self.model(batch_data)   #generate predictions\n","            predictions = predictions.squeeze(1)\n","\n","            loss = self.criterion(predictions.float(), batch_data['labels'].float())      #compute the loss\n","\n","            #backward pass\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            pred = (predictions > 0.0 ).detach().int().cpu().numpy()           #get class label\n","\n","            start = batch_id * batch_size\n","            end = start + batch_size\n","\n","            #concatenate the new tensors with the one computed in previous steps\n","            all_pred[start:end] = pred\n","            all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()\n","\n","            tot_loss += loss.item()    #accumulate batch loss\n","\n","\n","        acc, f1, prec, rec = metrics(all_targ,all_pred)\n","\n","        loss = tot_loss/(batch_id+1)    #mean loss\n","\n","        end_time = time.perf_counter()\n","\n","        return loss, acc, f1, prec, rec, end_time-start_time\n","\n","\n","    def eval_loop(self, dataloader : DataLoader):\n","\n","        batch_size = dataloader.batch_size\n","        dataset_size = len(dataloader.dataset)\n","\n","        start_time = time.perf_counter()\n","\n","        tot_loss = 0\n","\n","        #aggregate all the predictions and corresponding true labels (and claim ids) in tensors\n","        all_pred , all_targ = np.empty(dataset_size), np.empty(dataset_size)\n","\n","        self.model.eval()   #model in eval mode\n","\n","        with torch.no_grad(): #without computing gradients since it is evaluation loop\n","\n","            for batch_id, batch_data in enumerate(tqdm(dataloader)):\n","\n","                predictions : Tensor = self.model(batch_data)   #generate predictions\n","                predictions = predictions.squeeze(1)\n","\n","                loss = self.criterion(predictions.float(), batch_data['labels'].float())      #compute the loss\n","\n","                pred = (predictions > 0.0 ).detach().int().cpu().numpy()        #get class label\n","                start = batch_id * batch_size\n","                end = start + batch_size\n","\n","                #concatenate the new tensors with the one computed in previous steps\n","                all_pred[start:end] = pred\n","                all_targ[start:end] = batch_data['labels'].detach().cpu().numpy()\n","\n","                tot_loss += loss.item()   #accumulate batch loss\n","\n","        acc, f1, prec, rec = metrics(all_targ,all_pred)\n","\n","        loss = tot_loss/(batch_id+1)   #mean loss\n","\n","        end_time = time.perf_counter()\n","\n","        return loss, acc, f1, prec, rec, end_time-start_time\n","\n","\n","    def train_and_eval(self, train_loader, val_loader, num_epochs):\n","        \"\"\"\n","            Runs the train and eval loop and keeps track of all the metrics of the training model\n","        \"\"\"\n","        best_f1 = -1   #init best f1 score\n","\n","        for epoch in range(1, num_epochs+1): #epoch loop\n","\n","            start_time = time.perf_counter()\n","\n","            print(f'Starting epoch {epoch}')\n","\n","            train_metrics = self.train_loop(train_loader)\n","            val_metrics = self.eval_loop(val_loader)\n","\n","            end_time = time.perf_counter()\n","\n","            tot_epoch_time = end_time-start_time\n","\n","            train_epoch_loss, train_epoch_acc, train_epoch_f1, train_epoch_prec, train_epoch_rec, train_epoch_time = train_metrics\n","            val_epoch_loss, val_epoch_acc, val_epoch_f1, val_epoch_prec, val_epoch_rec, val_epoch_time = val_metrics\n","\n","            if val_epoch_f1 >= best_f1:\n","                best_f1 = val_epoch_f1\n","                if not os.path.exists(self.models_dir):\n","                    os.makedirs(self.models_dir)\n","                torch.save(self.\n","                           model.state_dict(),self.models_dir/ f'{self.model.name()}.pt')\n","\n","            # wandb logs\n","            wandb.log({'train/loss': train_epoch_loss, 'train/acc': train_epoch_acc, 'train/f1': train_epoch_f1,\n","                       'train/prec': train_epoch_prec, 'train/rec': train_epoch_rec, 'train/time': train_epoch_time,\n","                       'val/loss': val_epoch_loss, 'val/acc': val_epoch_acc, 'val/f1': val_epoch_f1,\n","                       'val/prec': val_epoch_prec, 'val/rec': val_epoch_rec, 'val/time' : val_epoch_time,\n","                       'lr': self.optimizer.param_groups[0]['lr'], 'epoch': epoch})\n","\n","            print(f'Total epoch Time: {tot_epoch_time:.4f}')\n","            print(f'Train Loss: {train_epoch_loss:.3f} | Train Acc: {train_epoch_acc*100:.2f}% | Train F1: {train_epoch_f1:.2f}')\n","            print(f'Val. Loss: {val_epoch_loss:.3f} | Val. Acc: {val_epoch_acc*100:.2f}% | Val. F1: {val_epoch_f1:.2f}')\n","\n","    def test(self, test_loader):\n","\n","        print('loading model state from folder')\n","        self.model.load_state_dict(torch.load(\"/content/drive/MyDrive/Major Project/Different combinations/models/TweetAndAccount_model.pt\"))\n","        print('loaded')\n","\n","        test_metrics = self.eval_loop(test_loader)\n","        test_epoch_loss, test_epoch_acc, test_epoch_f1, test_epoch_prec, test_epoch_rec, test_epoch_time = test_metrics\n","\n","        print(f'Test -> Loss: {test_epoch_loss:.3f} | Acc: {test_epoch_acc:.3f} | F1: {test_epoch_f1:.3f} | Prec: {test_epoch_prec:.3f} | Rec: {test_epoch_rec:.3f} ')\n","\n","    def predict_single_sample_from_test(self, test_loader, index):\n","        for batch_id, batch_data in enumerate(test_loader):\n","            if batch_id == index // test_loader.batch_size:\n","                sample_index_in_batch = index % test_loader.batch_size\n","                predictions = self.model(batch_data)\n","                predictions = predictions.squeeze(1)\n","                pred = (predictions > 0.0).detach().int().cpu().numpy()\n","                return pred[sample_index_in_batch]\n"],"metadata":{"id":"JsUVxLeazN5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = TweetAndAccount_model(emb_matrix, cfg, DEVICE)"],"metadata":{"id":"PFWjYUcQ5PVe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(model, device, criterion, optimizer)"],"metadata":{"id":"UErCDDsU2wox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet = dataset.iloc[301]['tweet']\n","label = dataset.iloc[301]['label']\n","\n","print(f\"tweet: {tweet}, label: {label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PeyD1hr77OU0","executionInfo":{"status":"ok","timestamp":1714899548497,"user_tz":-330,"elapsed":393,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"52a9727d-0790-41f8-f245-48c779ddb8c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tweet: @TimSeaw Yeah it is more like “we claimed it was a hotfix, but we added a submarine feature to allow us to have a public fight with Apple”\n"," @kumpera @RenatoFontes Haha good point!\n"," @rileytestut @altstoreio Some tuning there might be nice, agreed.\n"," @virtudude @novallkhan @github @natfriedman Also, here is my take: https://t.co/qovEKRTDYK\n"," @rileytestut @altstoreio I think your hoops are at the right point for mass security.\n"," @rileytestut @altstoreio Need an email address to share\n"," @RenatoFontes I also think my taxes are too high.  I should pay 0% :-)\n"," @virtudude @novallkhan @github @natfriedman I don’t envy Nat’s and Satya’s position.   I don’t think we should support ICE, but I don’t think those contracts can be broken without fatal wounds.  I don’t have any internal information, but I saw Novell be destroyed by shareholder pressure and that was kindergarten leagues.\n"," @RenatoFontes Wait until you hear what the commissions are for every good you buy every day\n"," @rileytestut I wrote a draft last night of a blog post, not sure if I will ever finish it, and needs to be edited/cleaned up, but if you don’t mind reading a dirty post I can share with you.\n",", label: 1.0\n"]}]},{"cell_type":"code","source":["index = 301  # Index of the sample you want to predict\n","predicted_label = trainer.predict_single_sample_from_test(test_loader, index)\n","print(f'Predicted label: {predicted_label}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hIAZGYU2hRW","executionInfo":{"status":"ok","timestamp":1714899545898,"user_tz":-330,"elapsed":5967,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"f36f0866-5718-4582-bfbd-8e317ccd268a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted label: 1\n"]}]},{"cell_type":"code","source":["# name = datetime.now(tz = pytz.timezone('Europe/Rome')).strftime(\"%d/%m/%Y %H:%M:%S\")\n","model = TweetAndAccount_model(emb_matrix, cfg, DEVICE)\n","trainer = Trainer(model, device, criterion, optimizer)  # Initialize the Trainer with your model, device, criterion, and optimizer\n","# trainer.train_and_eval(train_loader, val_loader, NUM_EPOCHS)  # Call train_and_eval method\n","# trainer.test(test_loader)"],"metadata":{"id":"i2Ra7BSiIniJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.test(test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvYgss3C1ERw","executionInfo":{"status":"ok","timestamp":1727598497277,"user_tz":-330,"elapsed":35458,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"73d7d80a-0875-42a0-9937-1cb1fd2e2888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading model state from folder\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-31-6ef04ede72a1>:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  self.model.load_state_dict(torch.load(\"/content/drive/MyDrive/Major Project/Different combinations/models/TweetAndAccount_model.pt\"))\n"]},{"output_type":"stream","name":"stdout","text":["loaded\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9/9 [00:21<00:00,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Test -> Loss: 0.471 | Acc: 0.731 | F1: 0.731 | Prec: 0.736 | Rec: 0.734 \n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["# **ENSEMBLE WITH XGBOOST**\n","\n"],"metadata":{"id":"oY6yMoDa-1jC"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Train-test split\n","X_train = dataset_df[dataset_df['split'] == 'train'][account_features]\n","y_train = dataset_df[dataset_df['split'] == 'train']['label']\n","\n","X_test = dataset_df[dataset_df['split'] == 'test'][account_features]\n","y_test = dataset_df[dataset_df['split'] == 'test']['label']\n","\n","# Initialize and fit the XGBoost classifier\n","xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=6, use_label_encoder=False)\n","xgb_model.fit(X_train, y_train)\n","\n","# Get probabilities and convert to predictions\n","xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n","xgb_preds = (xgb_probs > 0.5).astype(int)\n","\n","# Calculate accuracy, precision, recall, and F1 score\n","accuracy = accuracy_score(y_test, xgb_preds)\n","precision = precision_score(y_test, xgb_preds)\n","recall = recall_score(y_test, xgb_preds)\n","f1 = f1_score(y_test, xgb_preds)\n","\n","# Output the metrics\n","print(f\"XGBoost Accuracy: {accuracy}\")\n","print(f\"XGBoost Precision: {precision}\")\n","print(f\"XGBoost Recall: {recall}\")\n","print(f\"XGBoost F1 Score: {f1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtzad16O5UHu","executionInfo":{"status":"ok","timestamp":1727606810180,"user_tz":-330,"elapsed":1147,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"e69d3e5d-dd5f-4b19-e16b-57932db76853"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [10:47:42] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["XGBoost Accuracy: 0.778836987607245\n","XGBoost Precision: 0.7413010590015129\n","XGBoost Recall: 0.8892921960072595\n","XGBoost F1 Score: 0.8085808580858086\n"]}]},{"cell_type":"code","source":["def get_lstm_predictions(dataloader):\n","    model.eval()\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for batch_data in dataloader:\n","            output = model(batch_data)\n","            probs = torch.sigmoid(output).cpu().numpy()  # Get predicted probabilities\n","            predictions.append(probs)\n","\n","    return np.concatenate(predictions)"],"metadata":{"id":"C785HM3Z5qzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm_probs = get_lstm_predictions(test_loader)"],"metadata":{"id":"sSvTwfm45ayO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","lstm_probs = lstm_probs.ravel()\n","print(f\"Flattened LSTM Probabilities shape: {lstm_probs.shape}\")\n","\n","print(f\"XGBoost Probabilities shape: {xgb_probs.shape}\")\n","\n","ensemble_probs = (lstm_probs + xgb_probs) / 2  # Averaging probabilities\n","\n","ensemble_preds_binary = (ensemble_probs > 0.5).astype(int)\n","\n","print(f\"Shape of ensemble_preds_binary: {ensemble_preds_binary.shape}\")\n","\n","ensemble_accuracy = accuracy_score(y_test, ensemble_preds_binary)\n","print(f\"Ensemble Accuracy: {ensemble_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LSOqsJTN6xCO","executionInfo":{"status":"ok","timestamp":1727599265796,"user_tz":-330,"elapsed":441,"user":{"displayName":"Sakshi Patil","userId":"10640018083628777169"}},"outputId":"2a274df6-b03b-42d6-8939-01ff5a57b85b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Flattened LSTM Probabilities shape: (1049,)\n","XGBoost Probabilities shape: (1049,)\n","Shape of ensemble_preds_binary: (1049,)\n","Ensemble Accuracy: 0.8064823641563393\n"]}]}]}